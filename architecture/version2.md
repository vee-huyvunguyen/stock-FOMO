    *generated by GPT

# Project Overview

**Goal**  
Develop a platform that predicts and displays stock changes (up, down, percentage change) influenced by news articles, using sentiment analysis. The project aims to showcase expertise in data engineering, web scraping, and scalable architecture design.

**Primary Objectives**  
1. Provide insights into stock trends based on news sentiment.
2. Demonstrate relationships between stock performance and journalist or industry-specific news trends.
3. Showcase daily updates and real-time predictions.
4. Highlight engineering skills in web scraping, data pipeline orchestration, and advanced analytics.

---

## **Requirements**

### 1. **Functional Requirements**  
#### Core Features:  
1. **Stock Sentiment Analysis**:
   - Compute and display average news sentiment (positive, neutral, negative) per company.
   - Provide detailed sentiment insights per journalist and per news outlet.  
   - Highlight industry-level sentiment trends.
2. **Stock-Dependent Analysis**:  
   - Visualize the dependence of stock performance on:
     - News polarity trends.
     - Specific journalists' opinions.
   - Track relationships between industry sectors and aggregated sentiment.  
3. **Real-Time Updates**:  
   - Daily data updates and sentiment analysis pipelines.
4. **Dashboard Filtering**:  
   - Filter by stock ticker, news source, polarity, journalist, industry, and date range.  

---

### 2. **Technical Requirements**  
#### Data Management:  
1. Crawl news articles from multiple sources, including **CNBC**, **Reuters**, and **Fox News**.  
2. Maintain a PostgreSQL database for:  
   - Scraped raw data.  
   - Checkpoints (last scraped timestamps for each source).  
   - Derived sentiment weights and insights.  

#### Data Engineering Pipelines:  
1. **Web Scraping**:
   - Use **Crawlee (TypeScript)** for parallel and efficient scraping.
   - Extract sentiment indicators and metadata.  
2. **Data Orchestration**:  
   - Use **Apache Airflow** for workflow automation and retry mechanisms.  
3. **Batch Processing and Analytics**:
   - Leverage **PySpark** and **dbt** for cleaning, transformation, and deriving sentiment weights.  

#### Sentiment Analysis:  
- Custom algorithm for real-time inference using PySpark, based on sentiment weights.  
- Store word polarity weights in a database for efficient runtime lookups.  

#### Backend Services:  
- Expose RESTful APIs via **FastAPI**, enabling filtered queries for the dashboard.  

#### Frontend Dashboard:  
- Build an interactive web interface using **Streamlit** with comprehensive filtering options.

#### Integration:  
- Use **Polygon.io** for stock data ingestion:
  - Sync historical stock data to the PostgreSQL warehouse.
  - Right now, it will not Stream new stock price updates in real-time.
  - Only through PostgreSQL

---

## **Architecture Design Assumptions**  
1. **Scalability Assumption**:
   - The system will support at least 1,000 stocks and daily scraping of as much articles as `robots.txt` allowed (100k max).
   - Scaling is based on:
        - How much the file `robots.txt` allow
        - How much the proxies services allow.
2. **Availability Assumption**:  
   - The App currently serve local usage.
3. **Polygon.io API**:  
   - Assumed to provide reliable filtered batch data upon requests. 

---

## **Constraints**  
1. **Budget Constraint**:  
   - Use all open-source tools (e.g., Airflow, Spark), for local deployment.  
2. **Latency Constraint**:  
   - Ensure daily updates complete within 3 hours of article availability.
3. **External Dependencies**:  
   - Dependence on third-party APIs like **Polygon.io** for stock data and journalist databases for credibility scores.

---

## **Quality Attributes (Non-Functional Requirements)**  
1. **Scalability**:  
   - Use distributed tools (PySpark, Airflow) for batch processing and workflow automation.  
2. **Maintainability**:  
   - Ensure modular design via separated scraping, data pipeline, and API services.  
3. **Usability**:  
   - Provide intuitive filters and visualizations for non-technical stakeholders on Streamlit.  
4. **Performance**:  
   - Serve dashboard queries within <500ms response time.  
5. **Data Integrity**:  
   - Ensure accurate crawling and processing through validation pipelines.  

---

## **Proposed Tech Stack**  
- **Scraping**: Crawlee (TypeScript), Puppeteer, Cheerio.  
- **Pipeline Orchestration**: Apache Airflow.  
- **Batch Processing**: PySpark, dbt.  
- **Backend**: FastAPI (Python).  
- **Frontend**: Streamlit.  
- **Database**: PostgreSQL.  